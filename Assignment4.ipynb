{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DPS-2912/C/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION-1\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "class RidgeRegressionGD:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, reg_param=0.1):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.reg_param = reg_param\n",
        "        self.theta = None\n",
        "        self.bias = 0\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.theta = np.zeros(n_features)\n",
        "\n",
        "        for _ in range(self.n_iterations):\n",
        "            y_pred = np.dot(X, self.theta) + self.bias\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y)) + self.reg_param * self.theta\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            self.theta -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.theta) + self.bias\n",
        "\n",
        "    def cost_function(self, y_true, y_pred):\n",
        "        return np.mean((y_true - y_pred) ** 2) + self.reg_param * np.sum(self.theta ** 2)\n",
        "\n",
        "np.random.seed(42)\n",
        "X_train = np.random.rand(100, 7)\n",
        "y_train = np.random.rand(100)\n",
        "X_test = np.random.rand(20, 7)\n",
        "y_test = np.random.rand(20)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "reg_params = [1e-5, 1e-3, 0, 1]\n",
        "\n",
        "best_cost = float('inf')\n",
        "best_r2 = float('-inf')\n",
        "best_lr = None\n",
        "best_reg = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for reg in reg_params:\n",
        "        ridge_gd = RidgeRegressionGD(learning_rate=lr, n_iterations=1000, reg_param=reg)\n",
        "        ridge_gd.fit(X_train_scaled, y_train)\n",
        "\n",
        "        y_pred_train = ridge_gd.predict(X_train_scaled)\n",
        "        y_pred_test = ridge_gd.predict(X_test_scaled)\n",
        "\n",
        "        cost = ridge_gd.cost_function(y_train, y_pred_train)\n",
        "        r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "        if cost < best_cost and r2 > best_r2:\n",
        "            best_cost = cost\n",
        "            best_r2 = r2\n",
        "            best_lr = lr\n",
        "            best_reg = reg\n",
        "\n",
        "print(f\"Best Learning Rate: {best_lr}\")\n",
        "print(f\"Best Regularization Parameter: {best_reg}\")\n",
        "print(f\"Minimum Cost: {best_cost}\")\n",
        "print(f\"Maximum R2 Score: {best_r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn-1t7MVeGLz",
        "outputId": "77f3cf8b-94c4-4e02-dada-87fbc79ac72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Learning Rate: 0.01\n",
            "Best Regularization Parameter: 1e-05\n",
            "Minimum Cost: 0.06931256649186872\n",
            "Maximum R2 Score: 0.0181983828641622\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#QUESTION-2\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"https://drive.google.com/uc?export=download&id=1q0FKlrfHmsXqhUrDYh5K3P26biXEUcTs\")\n",
        "\n",
        "data = data.dropna(subset=['Salary'])\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "data[['Salary']] = imputer.fit_transform(data[['Salary']])\n",
        "\n",
        "categorical_features = ['League', 'Division', 'NewLeague']\n",
        "one_hot = OneHotEncoder()\n",
        "transformer = ColumnTransformer([('one_hot', one_hot, categorical_features)], remainder='passthrough')\n",
        "data_transformed = transformer.fit_transform(data)\n",
        "\n",
        "data_transformed = pd.DataFrame(data_transformed, columns=transformer.get_feature_names_out())\n",
        "\n",
        "salary_column = 'remainder__Salary'\n",
        "\n",
        "# Separate input and output features\n",
        "X = data_transformed.drop(salary_column, axis=1)\n",
        "y = data_transformed[salary_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Perform scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# Fit a Linear Regression model\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(X_train, y_train)\n",
        "\n",
        "# Fit a Ridge Regression model\n",
        "ridge_model = Ridge(alpha=0.5)\n",
        "ridge_model.fit(X_train, y_train)\n",
        "\n",
        "# Fit a LASSO Regression model\n",
        "lasso_model = Lasso(alpha=0.5,max_iter=10000)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Predict and evaluate Linear Regression model\n",
        "y_pred_linear = linear_model.predict(X_test)\n",
        "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
        "print(f'Linear Regression MSE: {mse_linear}')\n",
        "\n",
        "# Predict and evaluate Ridge Regression model\n",
        "y_pred_ridge = ridge_model.predict(X_test)\n",
        "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "print(f'Ridge Regression MSE: {mse_ridge}')\n",
        "\n",
        "# Predict and evaluate LASSO Regression model\n",
        "y_pred_lasso = lasso_model.predict(X_test)\n",
        "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "print(f'LASSO Regression MSE: {mse_lasso}')\n",
        "\n",
        "# Determine which model performs the best\n",
        "best_model = min((mse_linear, 'Linear'), (mse_ridge, 'Ridge'), (mse_lasso, 'LASSO'))\n",
        "print(f'The best model is {best_model[1]} with an MSE of {best_model[0]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r80fzUe3eD4a",
        "outputId": "a2ec94e1-5ee4-47ed-c63b-58ae7eddd514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression MSE: 128284.34549672344\n",
            "Ridge Regression MSE: 126665.75189802826\n",
            "LASSO Regression MSE: 126820.57587652761\n",
            "The best model is Ridge with an MSE of 126665.75189802826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s00-Z-vu3Nv1",
        "outputId": "2d5192ed-a8e6-4f5b-f60c-74403180552c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AtBat         0\n",
            "Hits          0\n",
            "HmRun         0\n",
            "Runs          0\n",
            "RBI           0\n",
            "Walks         0\n",
            "Years         0\n",
            "CAtBat        0\n",
            "CHits         0\n",
            "CHmRun        0\n",
            "CRuns         0\n",
            "CRBI          0\n",
            "CWalks        0\n",
            "League        0\n",
            "Division      0\n",
            "PutOuts       0\n",
            "Assists       0\n",
            "Errors        0\n",
            "Salary       59\n",
            "NewLeague     0\n",
            "dtype: int64\n",
            "Linear Regression RMSE: 358.1680408645131 R²: 0.290745185579814\n",
            "Ridge Regression RMSE: 357.10230176355935 R²: 0.2949597213851176\n",
            "LASSO Regression RMSE: 355.2648892609774 R²: 0.30219639867303383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:697: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.082e+06, tolerance: 4.367e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://drive.google.com/uc?export=download&id=16WWIwbnFZjahcCKNDkMPBq1gvZDV_lEY\"\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Check for null values\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Handling null values (drop rows with nulls for simplicity)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Convert categorical variables to numerical using one-hot encoding\n",
        "data = pd.get_dummies(data, drop_first=True)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop('Salary', axis=1)  # Assuming 'Salary' is the target\n",
        "y = data['Salary']\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "\n",
        "# Initialize models\n",
        "linear_model = LinearRegression()\n",
        "ridge_model = Ridge(alpha=0.1248 ,max_iter = 200 , tol = 1e-4 )\n",
        "\n",
        "\n",
        "# Fit the models\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "ridge_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Lasso\n",
        "\n",
        "# Initialize models\n",
        "linear_model = LinearRegression()\n",
        "lasso_model = Lasso(alpha=0.1248 , max_iter = 200 , tol = 1e-4)\n",
        "\n",
        "\n",
        "# Fit the models\n",
        "linear_model.fit(X_train_scaled, y_train)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Function to evaluate model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    return rmse, r2\n",
        "\n",
        "# Evaluate models\n",
        "linear_rmse, linear_r2 = evaluate_model(linear_model, X_test_scaled, y_test)\n",
        "ridge_rmse, ridge_r2 = evaluate_model(ridge_model, X_test_scaled, y_test)\n",
        "lasso_rmse, lasso_r2 = evaluate_model(lasso_model, X_test_scaled, y_test)\n",
        "\n",
        "# Print results\n",
        "print(\"Linear Regression RMSE:\", linear_rmse, \"R²:\", linear_r2)\n",
        "print(\"Ridge Regression RMSE:\", ridge_rmse, \"R²:\", ridge_r2)\n",
        "print(\"LASSO Regression RMSE:\", lasso_rmse, \"R²:\", lasso_r2)"
      ]
    }
  ]
}